{"teacher_coherence": -0.0040107506944416045, "teacher_top_tokens": [["rekao", "sad", "predsednik", "nakon", "predsednika", "zbog", "protiv", "izme\u0111u", "afp", "izjavio"], ["po\u0161to", "pitanju", "2017", "ovom", "veliki", "druge", "onda", "potpuno", "\u017eeli", "vidi"], ["rekao", "predsednik", "sad", "izme\u0111u", "zbog", "nakon", "biti", "images", "reuters", "izjavio"], ["tri", "sam", "jedna", "kad", "nego", "posle", "gotovo", "ima", "gde", "mnogo"], ["sad", "rekao", "predsednik", "predsednika", "zemlje", "tako\u0111e", "protiv", "izjavio", "nakon", "ne\u0107e"], ["bio", "ka\u017ee", "sam", "zbog", "bila", "bilo", "vi\u0161e", "oni", "nekoliko", "rekao"], ["sad", "predsednik", "rekao", "izme\u0111u", "biti", "nakon", "protiv", "tokom", "prema", "jo\u0161"], ["tri", "nego", "jedna", "posle", "svoju", "ima", "mogao", "kad", "gotovo", "toga"], ["tri", "nego", "kad", "gotovo", "ima", "svoju", "samo", "vi\u0161e", "jedna", "\u010detiri"], ["sam", "ka\u017ee", "ljudi", "smo", "bila", "ne\u0161to", "godina", "jedan", "vreme", "jer"], ["druge", "vidi", "slu\u010daju", "predsednika", "stvari", "nakon", "ovom", "zemlje", "rekla", "\u017eeli"], ["sam", "jo\u0161", "bio", "smo", "ka\u017ee", "bila", "images", "imao", "jer", "jedan"], ["pitanju", "ovom", "dodaje", "potpuno", "srbiji", "stvari", "2017", "puta", "druge", "vidi"], ["sam", "ne\u0161to", "godina", "jer", "srbiji", "bez", "nema", "ili", "ljudi", "sada"], ["srbiji", "sam", "dana", "images", "svi", "bbc", "srbije", "onda", "zbog", "jer"], ["sam", "ka\u017ee", "smo", "bio", "ne\u0161to", "jo\u0161", "pre", "vi\u0161e", "bbc", "bez"], ["srbije", "jer", "svi", "sam", "ka\u017ee", "bude", "bbc", "godina", "srbiji", "zbog"], ["sam", "bio", "ne\u0161to", "ono", "bila", "tako", "nisu", "imao", "zato", "getty"], ["tri", "posle", "nego", "kad", "jedna", "gotovo", "mogao", "\u010detiri", "toga", "dve"], ["\u017eeli", "2017", "2018", "me\u0111utim", "slu\u010daju", "pitanju", "novi", "nikada", "dodaje", "ova"]], "teacher_diversity": 0.43, "dataset": "ted_talks", "embedding_method": "Pre-trained embeddings", "model": "GMM", "topic_method": "tf", "language": "urdu", "dataset_length": 1315, "average_document_length": 53.0, "vocab_size": 238, "embedding_model": "BERT", "training_time": 20.480109214782715}